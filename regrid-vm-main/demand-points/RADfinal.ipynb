{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb5e9a7-efa8-4420-9d8c-b1ec2ec8cf19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T18:21:06.680593Z",
     "iopub.status.busy": "2022-06-15T18:21:06.680271Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/geopandas/_compat.py:111: UserWarning: The Shapely GEOS version (3.10.2-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-11:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-3:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/queues.py\", line 368, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/queues.py\", line 368, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/queues.py\", line 368, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/queues.py\", line 368, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/queues.py\", line 368, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/queues.py\", line 368, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/queues.py\", line 368, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/queues.py\", line 368, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/queues.py\", line 368, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/synchronize.py\", line 101, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/queues.py\", line 368, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/queues.py\", line 368, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/synchronize.py\", line 101, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/queues.py\", line 368, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/synchronize.py\", line 101, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/queues.py\", line 368, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/synchronize.py\", line 101, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/synchronize.py\", line 101, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/synchronize.py\", line 101, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/queues.py\", line 368, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/synchronize.py\", line 101, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/synchronize.py\", line 101, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/queues.py\", line 369, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/synchronize.py\", line 101, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/queues.py\", line 368, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/synchronize.py\", line 101, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/synchronize.py\", line 101, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/synchronize.py\", line 101, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/synchronize.py\", line 101, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/synchronize.py\", line 101, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/connection.py\", line 224, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/synchronize.py\", line 101, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/connection.py\", line 422, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/nhat/anaconda3/envs/py39/lib/python3.9/site-packages/multiprocess/connection.py\", line 387, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "for sf_code in ['TX', 'FL', 'CA']:\n",
    "    # sf_code = 'TX'\n",
    "    date=\"jun15\" # make dir if not yet exist\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import geopandas as gp\n",
    "    import h3\n",
    "    import h3pandas\n",
    "    from placekey.api import PlacekeyAPI\n",
    "    from keplergl import KeplerGl\n",
    "\n",
    "    from glob import glob\n",
    "    from multiprocess import Pool\n",
    "    from unsync import unsync\n",
    "    import gzip \n",
    "    from zipfile import ZipFile as ZF\n",
    "    from shapely.geometry import shape\n",
    "    from collections import Counter\n",
    "\n",
    "\n",
    "    # 50 states + DC + 6 territories (PR, VI, AS, GU, MP, UM)\n",
    "    SF57 = {'01': 'AL', '02': 'AK', '04': 'AZ', '05': 'AR', '06': 'CA', '08': 'CO', '09': 'CT', '10': 'DE', '11': 'DC', '12': 'FL', '13': 'GA', '15': 'HI', '16': 'ID', '17': 'IL', '18': 'IN', '19': 'IA', '20': 'KS', '21': 'KY', '22': 'LA', '23': 'ME', '24': 'MD', '25': 'MA', '26': 'MI', '27': 'MN', '28': 'MS', '29': 'MO', '30': 'MT', '31': 'NE', '32': 'NV', '33': 'NH', '34': 'NJ', '35': 'NM', '36': 'NY', '37': 'NC', '38': 'ND', '39': 'OH', '40': 'OK', '41': 'OR', '42': 'PA', '44': 'RI', '45': 'SC', '46': 'SD', '47': 'TN', '48': 'TX', '49': 'UT', '50': 'VT', '51': 'VA', '53': 'WA', '54': 'WV', '55': 'WI', '56': 'WY', '60': 'AS', '66': 'GU', '69': 'MP', '72': 'PR', '74': 'UM', '78': 'VI'}\n",
    "    SF57R = {value : key for (key, value) in SF57.items()}\n",
    "\n",
    "    def gen_h3_hex_vectors(lat, lon, res):\n",
    "        return h3.geo_to_h3(lat, lon, res)\n",
    "    gen_h3_hex = np.vectorize(gen_h3_hex_vectors)\n",
    "\n",
    "\n",
    "    attom_sel_cols = ['[ATTOM ID]',     \n",
    "        'LegalDescription', 'OwnerTypeDescription1', 'ParcelNumberRaw', \n",
    "        'PartyOwner1NameFull', 'PartyOwner2NameFull', \n",
    "        'PropertyAddressCity', \n",
    "        'PropertyAddressFull', \n",
    "        'PropertyAddressHouseNumber', 'PropertyAddressStreetDirection', 'PropertyAddressStreetName', \n",
    "        'PropertyAddressStreetPostDirection', 'PropertyAddressStreetSuffix', 'PropertyAddressUnitPrefix', 'PropertyAddressUnitValue', \n",
    "        'PropertyAddressZIP', \n",
    "        'PropertyLatitude', 'PropertyLongitude',]\n",
    "\n",
    "    # NOTE: geom = WTK string/object type; geometry = geopandas geometry type\n",
    "    CRS_REGRID_PARCELS = \"EPSG:4326\"\n",
    "\n",
    "    regrid_sel_cols = ['ll_uuid', \n",
    "        'address', \n",
    "       'address2', # i.e. alternative address for same parcel\n",
    "       # 'original_address', # confirmed: all useless when (~address & original_address)\n",
    "                   'legaldesc', 'parcelnumb', \n",
    "        'saddno', 'saddpref', 'saddstr', 'saddsttyp', 'saddstsuf', 'sunit' ,\n",
    "        'scity', 'szip', 'lat', 'lon', \n",
    "        'owner', 'owner2', ]\n",
    "\n",
    "    def owner_compare(df_compare, left, right):\n",
    "        # only keep alphanumeric values ; note the additions of symbols\n",
    "        keepchars = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\" + \",& \"\n",
    "        del_chars = ''.join(c for c in map(chr, range(1114111)) if not c in set(keepchars))\n",
    "        # convert uppercases to lowercases\n",
    "        keepchars_table = str.maketrans(keepchars, keepchars.lower(), del_chars)\n",
    "\n",
    "        df_compare['owners'] = df_compare[left].str.translate(keepchars_table).str.split(',|&| ')\n",
    "        df_compare['Owners'] = df_compare[right].str.translate(keepchars_table).str.split(',|&| ')\n",
    "\n",
    "        def owners_combine(owners, Owners):\n",
    "            if owners is not np.nan and Owners is not np.nan:\n",
    "                seta = {own for own in owners if len(own) > 1}\n",
    "                setb = {own for own in Owners if len(own) > 1}\n",
    "                return True if seta.intersection(setb) else False\n",
    "            return True\n",
    "        owners_combine_vectorize = np.vectorize(owners_combine)\n",
    "        return owners_combine_vectorize(df_compare['owners'], df_compare['Owners'])\n",
    "\n",
    "    attom_pk_maps = {\n",
    "        'aid' : 'query_id', # unique row id\n",
    "        'Lat' : 'latitude', \n",
    "        'Lon' : 'longitude', \n",
    "        'Address1' : 'street_address', # NOTE: using address1\n",
    "        'ACity' : 'city',\n",
    "        'AZip' : 'postal_code',\n",
    "    }\n",
    "\n",
    "    regrid_pk_maps = {\n",
    "        'rid' : 'query_id', # unique row id\n",
    "        'lat' : 'latitude', \n",
    "        'lon' : 'longitude', \n",
    "        'address1' : 'street_address', # NOTE: using address1\n",
    "        'scity' : 'city',\n",
    "        'szip' : 'postal_code', # e.g. 48103, 48104-3423\n",
    "    }\n",
    "\n",
    "    pk_api1 = PlacekeyAPI(\"NDd0VsiPKznvm89NLNj9DHrtU3GVzZ1h\" )\n",
    "    pk_api2 = PlacekeyAPI(\"50L6WAI5cSUz1gJ3qzr2AwQxANKP3Ty1\" )\n",
    "\n",
    "    #  https://github.com/alex-sherman/unsync#multi-threading-an-io-bound-function\n",
    "    # Iterative API calls= slow! Use unsync that simplifies ThreadPoolExecutor's async tasks\n",
    "    # @unsync convert a regular synchronous func into a threaded Unfuture\n",
    "    def pk_call(df, sf_code, maps, orig_id, pk_api):\n",
    "        def gen_placekey_df(df, maps):\n",
    "            keep_cols = list(maps.values())\n",
    "            # only need placekey cols\n",
    "            # drop columns with names similar to Placekey required columns\n",
    "            df = df.drop(columns=keep_cols, \n",
    "                         errors='ignore').rename(columns=maps)[keep_cols]\n",
    "\n",
    "            # zipcode = 00000 means null; ATTOM zipcode could be a float, or None, \"\"\n",
    "            # so, need a placeholder value to convert to int, zfill, back to np.nan\n",
    "            NAN_ZIPS = \"00000\"\n",
    "            df['postal_code'] = df['postal_code'].fillna(NAN_ZIPS).astype(int).astype(str).str.zfill(5).replace(NAN_ZIPS, np.nan)\n",
    "            # print(\"Generated pk df \", df.shape)\n",
    "\n",
    "            # OPTIONAL CLEANING\n",
    "            possible_bad_values = [\"\", \" \", \"null\", \"Null\", \"None\", \"nan\", \"Nan\"]  \n",
    "            for bad_value in possible_bad_values:\n",
    "                df = df.replace(bad_value, np.nan)\n",
    "            # replace NoneType with np.nan    \n",
    "            df.fillna(np.nan, inplace=True)\n",
    "            return df   \n",
    "\n",
    "        # Synchronous functions can be made to run asynchronously by executing them in a concurrent.ThreadPoolExecutor. \n",
    "        # This can be easily accomplished by marking (decorating) the regular function @unsync.\n",
    "        @unsync\n",
    "        # Placekey API lookup function\n",
    "        def pk_lookup(df, pk_api):\n",
    "            # add missing hard-coded columns (str type)\n",
    "            df['iso_country_code'] = 'US' \n",
    "            # sf_code has GLOBAL SCOPE\n",
    "            df['region'] = sf_code.upper()    \n",
    "            df = json.loads(df.to_json(orient='records'))\n",
    "            # Rate limit: 100 bulk req per min x 100 addrs per bulk req\n",
    "            # i.e. 10,000 addr per min \n",
    "            responses =  pk_api.lookup_placekeys(df, \n",
    "                                            strict_address_match=False,\n",
    "                                            strict_name_match=False, \n",
    "                                            # verbose=True,\n",
    "                                           )\n",
    "            # Clean the responses\n",
    "            # print(\"number of requests sent: \", len(df))\n",
    "            # print(\"total queries returned:\", len(responses))\n",
    "            # filter out invalid responses\n",
    "            responses_cleaned = [resp for resp in responses if 'query_id' in resp]\n",
    "            # print(\"total successful responses:\", len(responses_cleaned))\n",
    "            # print(f\"COMPLETED querying placekey api. Total queries returned: {sf_code}, {len(responses)}\")\n",
    "            return pd.read_json(json.dumps(responses_cleaned), dtype={'query_id':str})\n",
    "\n",
    "\n",
    "        # TODO opt: groupby address1+hex7: avoid duplicate Placekey API request!\n",
    "        # assumption: there should be no duplicate address1 within same ahex7 (within 5km2)\n",
    "        # will not filter: same addr & adjacent ahex7 - placekey API will verify!\n",
    "        # FAILED: concat address with zip, eliminate repeated address (https://www.quora.com/Do-any-two-locations-with-the-same-street-address-also-have-the-same-ZIP-code)\n",
    "        # also, groupby fails when zip is null!\n",
    "        df_pk = gen_placekey_df(df, maps)\n",
    "\n",
    "        # API REQUEST\n",
    "        pk_res_unfuture =  pk_lookup(df_pk.copy(), pk_api) # # (148_487, 6) around 15min\n",
    "        pk_res = pk_res_unfuture.result() # blocking \n",
    "        # Show API responses errors\n",
    "        # if \"error\" in set(pk_res):\n",
    "        #     print(orig_id, \"# responses: \", pk_res.shape, \"; responses errors: \")\n",
    "        #     print(pk_res.error.value_counts())\n",
    "\n",
    "        # split into 2 components -- only interest in results with pkwhat\n",
    "        # this also filters rows with error i.e. no placekey\n",
    "        if \"placekey\" in set(pk_res):\n",
    "            pk_res[['pkwhat', 'pkwhere']] = pk_res.placekey.str.split(\n",
    "                \"@\", expand=True).replace(\"\", np.nan)\n",
    "        else:\n",
    "            pk_res[['placekey', 'pkwhat', 'pkwhere']] = np.nan\n",
    "\n",
    "        # Save API responses!!\n",
    "        pk_res.reset_index(drop=True).to_feather(f'placekeyed/{orig_id}_{sf_code}.ftr')\n",
    "\n",
    "        # keep only results whith pkwhat components\n",
    "        pk_res = pk_res[pk_res.pkwhat.notna()]\n",
    "        # Merge with original df; keep only rows with placekey results\n",
    "        df = pd.merge(df, pk_res, left_on = orig_id, \n",
    "          right_on=\"query_id\", how='inner').drop(columns= ['query_id', 'error'], errors='ignore')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def regrid_landuse_classifier(acti, func, struc, site):\n",
    "        # REGRID LAND USE CLASSIFICATION\n",
    "        if 1 in (acti, func, struc):\n",
    "            return 'resi'\n",
    "        if 2 in (acti, func, struc) or func in set('357') or acti in set('357'):\n",
    "            return 'biz'\n",
    "        if 8 in (acti, struc) or func == 9: \n",
    "            return 'farm'\n",
    "        if 4 in (acti, func) or 6 in (acti, func) or struc in set('34567'):\n",
    "            return 'CAI'\n",
    "        if site != 6: # no developed site on land\n",
    "            return 'vacland'    \n",
    "        return 'rem' # ALL OTHER LANDUSE GROUPS   \n",
    "\n",
    "\n",
    "    def rad_fields_vec(rid, aid, address1, addressSub, Address1, AddressSub, lat, lon, Lat, Lon, owner, owner2, Owner, Owner2):\n",
    "        pId = rId = pAddress1 = pAddressSub = pOwner = pLat = pLon = np.nan\n",
    "\n",
    "        rlist = [rid, address1, addressSub, lat, lon, owner, owner2]\n",
    "        for r, addr1, addrSub, la, lo, own, own2 in zip(*rlist):\n",
    "            pId = rId = r\n",
    "            if addr1 is not None and addr1 == addr1: # not None, not na\n",
    "                pId = rId = r\n",
    "                pAddress1, pAddressSub = addr1, addrSub\n",
    "            if la is not None and lo is not None and la == la and lo == lo:\n",
    "                pLat, pLon = la, lo\n",
    "            if own is not None and own == own:\n",
    "                pOwner = own\n",
    "            elif own2 is not None and own2 == own2:\n",
    "                pOwner = own2\n",
    "            if np.nan not in (pId, rId, pAddress1, pAddressSub, pOwner, pLat, \n",
    "               pLon):\n",
    "                return \"\\t\".join([str(k) for k in [pId, rId, pAddress1, pAddressSub, pOwner, pLat, pLon]])\n",
    "\n",
    "        alist = [aid, Address1, AddressSub, Lat, Lon, Owner, Owner2]\n",
    "        for a, addr1, addrSub, la, lo, own, own2 in zip(*alist):\n",
    "            if pId != pId: # if have seen no regrid parcel \n",
    "                pId = a\n",
    "            if pAddress1 != pAddress1 and addr1 is not None and addr1 == addr1: # not na\n",
    "                pId = a # rId would still be Nan, or equal to a previous rid\n",
    "                pAddress1, pAddressSub = addr1, addrSub\n",
    "            if pLat != pLat and pLon != pLon and la is not None and lo is not None and la == la and lo == lo:\n",
    "                pLat, pLon = la, lo\n",
    "            if pOwner != pOwner and own is not None and own == own:\n",
    "                pOwner = own\n",
    "            elif pOwner != pOwner and own2 is not None and own2 == own2:\n",
    "                pOwner = own2\n",
    "            if np.nan not in (pId, rId, pAddress1, pAddressSub, pOwner, pLat, \n",
    "               pLon):\n",
    "                return \"\\t\".join([str(k) for k in [pId, rId, pAddress1, pAddressSub, pOwner, pLat, pLon]])\n",
    "        return \"\\t\".join([str(k) for k in [pId, rId, pAddress1, pAddressSub, pOwner, pLat, pLon]])\n",
    "\n",
    "    def rad_landuse_classifier_single(alanduse, rlanduse):\n",
    "        # Higher to lower prioritization\n",
    "        if alanduse in ('CAI', 'biz', 'resi'): # also in order of priority\n",
    "            return alanduse\n",
    "        if rlanduse in ('CAI', 'biz', 'resi'): # also in order of priority\n",
    "            return rlanduse\n",
    "        for weak in ('farm', 'vacland', 'rem'): # also in order of priority\n",
    "            if weak in set((alanduse, rlanduse)):\n",
    "                return weak\n",
    "\n",
    "    SORT_ORDER = {'CAI': 10, 'biz': 9, 'resi': 8, 'farm': 7, 'vacland': 6, 'rem': 5, np.nan: 4}\n",
    "    def rad_landuse_classifier_many(uses):\n",
    "        # RAD LAND USE CLASSIFICATION: reconciling [many] ATTOM <> [many] REGRID\n",
    "        c = Counter(uses).most_common()\n",
    "        # note: negative signs! descending by counts, then descending by SORT_ORDER (for breaking tie)\n",
    "        landuse_orders = sorted(c, key = lambda x: (-x[1], -SORT_ORDER[x[0]]))\n",
    "        for value, count in landuse_orders:\n",
    "            if value not in ('vacland', 'rem', np.nan):\n",
    "                return value\n",
    "        if 'vacland' in set([k[0] for k in landuse_orders]): \n",
    "            return 'vacland'\n",
    "        return 'rem'\n",
    "\n",
    "    def addr_fields_join(*args):\n",
    "        args = [str(i).strip() for i in args]\n",
    "        return \" \".join(i for i in args if (i and i != 'nan' and i != 'None'))\n",
    "    addr_fields_join_vectorize = np.vectorize(addr_fields_join)\n",
    "\n",
    "\n",
    "    # only keep alphanumeric values; use to clean:\n",
    "    # ['LegalDescription', 'ParcelNumberRaw'] and ['legaldesc', 'parcelnumb']\n",
    "    keepchars = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
    "    del_chars = ''.join(c for c in map(chr, range(1114111)) if not c in set(keepchars))\n",
    "    # convert lowercases to uppercases\n",
    "    legal_number_translate_table = str.maketrans(keepchars, keepchars.upper(), del_chars)\n",
    "\n",
    "\n",
    "    government = [\"Government\", \"Justice\", \"Library\", \"Department\", \"Administration\",\"Dept of\", \"City of\", \"Dept of\", \n",
    "                  \"Town of\", \"Commission\", \"National Foundation\",  \"Child Abuse\", \"Courthouse\", \"Department of \", \n",
    "                  \"Leadership\", \"Authority\",\"Adoptions\", \"Prison\",\"Veteran\", \"Chamber of\", \"Municipal\",\"Child Support\"]\n",
    "    religious = [\"Church\", \"Baptist\",\"Baptist Association\",\"Christ\",\"Family Harvest\"]\n",
    "    community = [\"Communications Workers Of America\", \"Veteran\", \"Community Center\", \"Community\"]\n",
    "    education = [\"High School\", \"Elementary School\", \"Education\",\"College\", \n",
    "                        \"University\", \"Pre-School\",\"Middle School\", \"School\"]\n",
    "    outdoor = [\"Garden\", \"Zoo\", \"Park\", \"Recreation\", \"Stadium\", \"Memorial\", \n",
    "               \"Outlook\", \"Overlook\", \"Pavilion\", \"Square\", \"Field\", \"Resort\"]\n",
    "\n",
    "    stopcats = ['PERSONAL SERVICES',  'EATING - DRINKING', 'SHOPPING', \n",
    "           'AUTOMOTIVE SERVICES',  'BANKS - FINANCIAL', 'PET SERVICES']\n",
    "    keywords = {\n",
    "        \"health\": [\"Medical Center\",\"Hospital\",\"Clinic\",\"Family Care\", \n",
    "                     # \"Rehab\"\n",
    "                    ],\n",
    "        \"education\": education, \"government\": government, \"community\": community, \n",
    "        \"religious\": religious, \"outdoor\": outdoor,\n",
    "    }\n",
    "\n",
    "    STOP_PARTS = [\"LLC\", \"Llc\", \"inc\", \"Inc\", \"INC\"]\n",
    "    stopwords = {\n",
    "        \"health\":     STOP_PARTS + [\"Hospitality\", \"Hospitalities\", \"Schools\", \"Animal\", \"Pet\"],\n",
    "        \"education\":  STOP_PARTS + [\"Hospital\"],\n",
    "        \"government\": STOP_PARTS + [\"Department Store\"],\n",
    "        \"community\":  STOP_PARTS,\n",
    "        \"religious\":  STOP_PARTS,\n",
    "        \"outdoor\":    STOP_PARTS,\n",
    "    }\n",
    "    caikeys = {\n",
    "        \"health\": [\"Medical Center\",\"Hospital\",\"Family Care\"],\n",
    "        \"education\": education, \"government\": government, \"community\": community, \n",
    "        \"religious\": religious, \"outdoor\": outdoor,\n",
    "    }\n",
    "\n",
    "    def getcai(dfinput, category):\n",
    "        #STEP 1. use the name key word\n",
    "        df = dfinput[dfinput[\"BUSNAME\"].str.contains(\"|\".join(keywords[category]))]\n",
    "        #2.  remove the ones that contain the stop word\n",
    "        df = df[~df[\"BUSNAME\"].str.contains(\"|\".join(stopwords[category]))]\n",
    "        df = df[~df[\"CATEGORY\"].isin(stopcats)]\n",
    "        # 3. Find the CAI\n",
    "        caidf = df[df[\"BUSNAME\"].str.contains(\"|\".join(caikeys[category]))]\n",
    "        return caidf, df\n",
    "\n",
    "\n",
    "    def init():\n",
    "        sf = SF57R[sf_code]\n",
    "        list_of_feathers = glob(f\"attom/data/attom-feather/{sf_code.upper()}_*.ftr\")#[-1:]\n",
    "        # print(len(list_of_feathers)) \n",
    "        with Pool(16) as pool:\n",
    "            attom = pool.map(lambda feather_file: pd.read_feather(feather_file, columns=attom_sel_cols), list_of_feathers)\n",
    "            attom = pd.concat(attom, ignore_index=True)#.head(200)\n",
    "        attom = attom.rename(columns= {\n",
    "            '[ATTOM ID]': 'aid',\n",
    "            'PropertyAddressFull' : 'Address',\n",
    "            'LegalDescription' : 'Legal', \n",
    "            'ParcelNumberRaw' : 'Numb', \n",
    "            'PartyOwner1NameFull' : \"Owner\",\n",
    "            'PartyOwner2NameFull' : 'Owner2',\n",
    "            'PropertyAddressCity' : \"ACity\",\n",
    "            'PropertyAddressZIP' : 'AZip',\n",
    "            'PropertyLatitude' : 'Lat',\n",
    "            'PropertyLongitude' : 'Lon', \n",
    "        })\n",
    "        attom.shape # (3_330_480, 18) in 15sec; DC = (213_079, 18) 2sec\n",
    "        # Address1 and AddressSub columns: 10sec\n",
    "        attom['Address1'] = addr_fields_join_vectorize(attom.PropertyAddressHouseNumber,\n",
    "            attom.PropertyAddressStreetDirection,\n",
    "            attom.PropertyAddressStreetName,\n",
    "            attom.PropertyAddressStreetSuffix,\n",
    "            attom.PropertyAddressStreetPostDirection)\n",
    "\n",
    "        attom['AddressSub'] = addr_fields_join_vectorize(attom.PropertyAddressUnitPrefix,\n",
    "            attom.PropertyAddressUnitValue,)\n",
    "\n",
    "        for col in ['Legal', 'Numb']:\n",
    "            attom[col] = attom[col].str.translate(legal_number_translate_table)\n",
    "\n",
    "        for bad_value in [\"\", \"None\"]:\n",
    "            attom.replace(bad_value, np.nan, inplace = True)\n",
    "        attom.fillna(np.nan, inplace=True)  # replace python NoneType with np.nan\n",
    "\n",
    "        cols = [\"Lat\", \"Lon\"]\n",
    "        attom[cols] = attom[cols].astype(float)\n",
    "\n",
    "        # Hotfix bad raw data = If Address1 null, set addr = address1\n",
    "        attom['Address1'] = np.where(\n",
    "            attom.Address.notna() & attom.Address1.isna(), \n",
    "            attom['Address'], attom['Address1'])\n",
    "        # If addr1 = addr, set addr2 = empty\n",
    "        attom['AddressSub'] = np.where(\n",
    "            attom.Address == attom.Address1, \n",
    "            np.nan, attom['AddressSub'])\n",
    "\n",
    "\n",
    "        # Sanity check: when Address is null, no other addr subfields exist!\n",
    "        cols = [\n",
    "            'Address','Address1', 'AddressSub',\n",
    "            'PropertyAddressHouseNumber','PropertyAddressStreetDirection',\n",
    "            'PropertyAddressStreetName','PropertyAddressStreetPostDirection',\n",
    "            'PropertyAddressStreetSuffix','PropertyAddressUnitPrefix','PropertyAddressUnitValue',\n",
    "        ]\n",
    "        check_address = set(attom.query(\"Address != Address\")[cols].count()) == {0} \n",
    "        if not check_address:\n",
    "            print(\"ALERT: ATTOM df: when Address is null, no other addr subfields should exist!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        list_of_regrid_feathers = glob(f\"attom/data/regrid-feather/{sf_code.lower()}_*.ftr\")#[-1:]\n",
    "        with Pool(16) as pool:\n",
    "            regrid = pool.map(lambda feather_file: pd.read_feather(feather_file, columns=regrid_sel_cols), list_of_regrid_feathers)\n",
    "            regrid = pd.concat(regrid, ignore_index=True)#.head(200) \n",
    "        # when sf_code = DC: no parcel has legaldesc\n",
    "        regrid[['legaldesc', 'parcelnumb']] = regrid[['legaldesc', 'parcelnumb']].astype(str) \n",
    "\n",
    "        regrid.rename(columns={'ll_uuid' : 'rid', 'sunit' : 'addressSub'}, inplace=True)\n",
    "        regrid.fillna(np.nan, inplace=True) \n",
    "\n",
    "        # Fix small Bad data#1: paste address2 to adress (7, 17)\n",
    "        regrid['address'] = np.where(regrid['address'].isna() & regrid['address2'].notna(),\n",
    "                                     regrid['address2'], regrid['address'])\n",
    "        mask = regrid.address.isin(['NO ADDRESS ASSIGNED BY COUNTY', 'No Situs Address'])\n",
    "        # Fix bad bad data#2: hardcoded empty addresses\n",
    "        regrid.loc[mask, \n",
    "            ['address', 'addressSub', 'saddno', 'address2',  \n",
    "             'saddpref', 'saddstr', 'saddsttyp', 'saddstsuf' ]] = np.nan\n",
    "        regrid.shape # (3330480, 18) in 15sec ## NEW as of Jun3, (3_219_695, 17); DC = (137_403, 17)\n",
    "\n",
    "        # Assemble REGRID address fields\n",
    "        regrid['address1'] = addr_fields_join_vectorize(\n",
    "            regrid.saddno, regrid.saddpref, regrid.saddstr, regrid.saddsttyp, regrid.saddstsuf)\n",
    "\n",
    "        regrid.address1.replace(\"\", np.nan, inplace=True)\n",
    "        # Hotfix inconsistent address fields\n",
    "        regrid.loc[regrid.address.isna() & regrid.address1.notna(), \"address\"] = regrid.address1\n",
    "        regrid.loc[regrid.address.notna() & regrid.address1.isna() & regrid.address1.isna(), \n",
    "                  ['address', 'address1', 'addressSub',]] = np.nan\n",
    "\n",
    "        regrid['szip'] = regrid.szip.str.replace(r'\\D+', '', regex = True).str[:5]\n",
    "        regrid.szip.mask(regrid.szip.str.len() != 5, inplace=True)\n",
    "\n",
    "        # Sanity check: when Address is null, no other addr subfields exist!\n",
    "        cols = ['address', 'address1', 'addressSub', 'saddno', 'saddpref', \n",
    "                            'saddstr', 'saddsttyp', 'saddstsuf']\n",
    "        check_address = set(regrid.query(\"address != address\")[cols].count()) == {0} \n",
    "        if not check_address:\n",
    "            print(\"ALERT: REGRID df: when Address is null, no other addr subfields should exist!\")\n",
    "\n",
    "        # Coords should have float types\n",
    "        cols = ['lat', 'lon',]\n",
    "        regrid[cols] = regrid[cols].astype(float)\n",
    "\n",
    "        for col in ['legaldesc', 'parcelnumb']:\n",
    "            regrid[col] = regrid[col].str.translate(legal_number_translate_table)\n",
    "\n",
    "        # At this point, legaldesc and parcelnumb could be \"\"\n",
    "        regrid.replace(\"\", np.nan, inplace=True)\n",
    "\n",
    "        all_possible_bad_values = [\"\", \" \", \"null\", \"Null\", \"None\", \"NONE\", \"none\", \"nan\", \"Nan\"] \n",
    "        for bad_value in all_possible_bad_values:\n",
    "            regrid.replace(bad_value, np.nan, inplace = True)\n",
    "\n",
    "        if regrid[['lat', 'lon']].isna().sum().sum() != 0:\n",
    "            print(f\"NOTE: {sf_code} regrid_df contains {regrid[['lat', 'lon']].isna().sum().mean()} null coords lat/lon\")\n",
    "\n",
    "\n",
    "        # ADD h3 HEX COLUMNS\n",
    "        for res in [7, 13]:\n",
    "            # res 7 = length=15, last 6 chars = ffffff; area~5 km2\n",
    "            # MO 24sec\n",
    "            attom[f'ahex{res}']= gen_h3_hex(attom['Lat'], attom['Lon'],  res)\n",
    "            regrid[f'rhex{res}']= gen_h3_hex(regrid['lat'], regrid['lon'],  res)\n",
    "\n",
    "        attom_no_coords = attom.groupby(\"ahex7\").size()[0] # 6669\n",
    "        if attom_no_coords:\n",
    "            print(f\"NOTE: ATTOM contains {attom_no_coords} null lat/lon rows, whose h3hex = '0'\")\n",
    "\n",
    "        # dup columns check\n",
    "        if (set(attom.columns.duplicated()), set(regrid.columns.duplicated())) != ({False}, {False}):\n",
    "            print(\"ALERT: attom/regrid contains duplicate column names\")\n",
    "\n",
    "\n",
    "        return regrid, attom\n",
    "\n",
    "    def legal(attom, regrid):\n",
    "        # MATCH 1 - uniq legal description\n",
    "        # IMPORTANT: first, drop the duplicate values\n",
    "        regrid_legal_uniq = regrid.drop_duplicates(subset='legaldesc', keep=False)\n",
    "        attom_legal_uniq = attom.drop_duplicates(subset='Legal', keep=False)\n",
    "        legal_matched = attom_legal_uniq.merge(regrid_legal_uniq, left_on = 'Legal', right_on = 'legaldesc', how = 'inner')\n",
    "        del regrid_legal_uniq, attom_legal_uniq\n",
    "        # MO (1_620_809, 42), as of Jun3 (1_604_138, 42)\n",
    "        # DC (0, 42), because REGRID DC has no legaldesc\n",
    "        legal_matched.shape \n",
    "\n",
    "        legal_diff = pd.DataFrame().reindex_like(legal_matched)\n",
    "        if len(legal_matched):\n",
    "            # MO = 13sec\n",
    "            # Create new column that compares owner <> Owner\n",
    "            legal_matched['owner_compare'] = owner_compare(legal_matched, 'owner', 'Owner')\n",
    "            legal_diff = legal_matched.query(\n",
    "                \"parcelnumb.notna() & Numb.notna() & parcelnumb != Numb\" \n",
    "                \"& address.notna() & Address.notna() & address != Address\"\n",
    "                \"& address1.notna() & Address1.notna() & address1 != Address1\"\n",
    "                \"& rhex7 != '0' & ahex7 != '0' & rhex7 != ahex7\" \n",
    "                \"& owner.notna() & Owner.notna() & (~owner_compare)\"    \n",
    "            , engine = \"python\")\n",
    "\n",
    "            print(\"legal_diff shape \", legal_diff.shape) # (87, 45), as of Jun3 (102, 45)\n",
    "            legal_diff[[ 'owner', 'owner2',  'Owner', 'Owner2', 'address1', 'Address1', 'parcelnumb', 'Numb',\n",
    "                        ]].sample(3)    \n",
    "        print(f\"COMPLETED legal matching {sf_code}\")\n",
    "        return legal_matched, legal_diff\n",
    "\n",
    "    def numb(attom, regrid, legal_matched, legal_diff):\n",
    "        # MATCH 2 - uniq parcel number\n",
    "        # attom2 and regrid2: all rows that have not been legal matched!\n",
    "        attom2 = attom[~attom.aid.isin(set(legal_matched.aid))]\n",
    "        regrid2 = regrid[~regrid.rid.isin(set(legal_matched.rid))]\n",
    "        regrid_parcelnumb = regrid2.drop_duplicates(subset='parcelnumb', keep=False)\n",
    "        attom_parcelnumb = attom2.drop_duplicates(subset='Numb', keep=False)\n",
    "        num_matched = attom_parcelnumb.merge(regrid_parcelnumb, left_on = 'Numb', right_on = 'parcelnumb', how = 'inner')\n",
    "        del regrid2, attom2, regrid_parcelnumb, attom_parcelnumb\n",
    "        # jun3 ((3219695, 20), (3330480, 22), (1604138, 45), (1207963, 42))\n",
    "        regrid.shape, attom.shape, legal_matched.shape, num_matched.shape\n",
    "\n",
    "        num_diff = pd.DataFrame().reindex_like(num_matched)\n",
    "        if len(num_matched):\n",
    "            num_matched['owner_compare'] = owner_compare(num_matched, 'owner', 'Owner')\n",
    "            # num_matched.query(\"owner_compare == False\").shape # (2918, 45)\n",
    "\n",
    "            num_diff = num_matched.query(\n",
    "                   \"address.notna() & Address.notna() & address != Address\" # (86667, 42)\n",
    "                   \"& address1.notna() & Address1.notna() & address1 != Address1\" \n",
    "                    \"& rhex7 != '0' & ahex7 != '0' & rhex7 != ahex7\" \n",
    "                    \"& owner.notna() & Owner.notna() & (~owner_compare)\"      \n",
    "            , engine = \"python\")\n",
    "            # print(f\"{num_diff.shape=}\") # (112, 45); jun3 (140, 45)\n",
    "\n",
    "        # SAVE CHECKPOINTS    \n",
    "        ! mkdir -p temp/{date}\n",
    "        if len(legal_matched): legal_matched.reset_index(drop=True).to_feather(f'temp/{date}/{sf_code}_legal.ftr')\n",
    "        if len(num_matched): num_matched.reset_index(drop=True).to_feather(f'temp/{date}/{sf_code}_num.ftr')\n",
    "        if len(legal_diff): legal_diff.reset_index(drop=True).to_feather(f'temp/{date}/{sf_code}_legaldiff.ftr')\n",
    "        if len(num_diff): num_diff.reset_index(drop=True).to_feather(f'temp/{date}/{sf_code}_numdiff.ftr')\n",
    "        print(f\"COMPLETED parcel-number matching {sf_code}\")\n",
    "        return num_matched, num_diff\n",
    "\n",
    "\n",
    "    def placekey_query(attom, regrid, legal_matched, legal_diff, num_matched, num_diff):\n",
    "        # MATCH 3 - PLACEKEY API\n",
    "        attom3 = attom[~attom.aid.isin(pd.concat([legal_matched.aid, num_matched.aid]))] \n",
    "        attom3.shape # (82080, 22)\n",
    "\n",
    "        attom3 = attom[~attom.aid.isin(pd.concat([legal_matched.aid, num_matched.aid]))] # concat 2 series\n",
    "        regrid3 = regrid[~regrid.rid.isin(pd.concat([legal_matched.rid, num_matched.rid]))] # concat 2 series\n",
    "        # Inner-join columns\n",
    "        attom3 = pd.concat([attom3, legal_diff, num_diff], join='inner')\n",
    "        regrid3 = pd.concat([regrid3, legal_diff, num_diff], join='inner')\n",
    "        # print(\"attom3 regrid3 including na-Address1\", attom3.shape, regrid3.shape)\n",
    "\n",
    "        # CONFIRMED separately: without address (or address1), there would be no pkwhat \n",
    "        # i.e. filter for parcels with non-empty addresses only (We dont need pkwhere, as it is just h3 @res10)\n",
    "        attom3 = attom3[attom3.Address1.notna()]\n",
    "        regrid3 = regrid3[regrid3.address1.notna()]\n",
    "        # print(\"attom3 regrid3 with valid Address1: \", attom3.shape, regrid3.shape)\n",
    "        shared_cols = set(attom3).intersection(set(regrid3))\n",
    "        if len(shared_cols):\n",
    "            print(\"ALERT: expected empty shared cols: \", shared_cols)\n",
    "\n",
    "        # BOTTLE NECK!!!!!! 40 MIN for MISSOURI!\n",
    "        # for paralleling api calls/network ops, use async\n",
    "        print(f\"STARTING QUERYING PLACEKEY API: {sf_code}, attom3 {len(attom3)}, and regrid3 {len(regrid3)}\")\n",
    "        pkA = pk_call(attom3, sf_code, attom_pk_maps, 'aid', pk_api1)\n",
    "        # print(f\"STARTING QUERYING PLACEKEY API: {sf_code}, regrid3 {len(regrid3)}\")\n",
    "        pkR = pk_call(regrid3, sf_code, regrid_pk_maps, 'rid', pk_api2)\n",
    "        # Unfuture.result() is a blocking operation except ...\n",
    "        # pkA = pkA_unfuture.result()\n",
    "        # pkR = pkR_unfuture.result()\n",
    "        # pkA.shape, pkR.shape # ((253555, 25), (170883, 23)); jun3 ((254_306, 25), (172_566, 23))\n",
    "        return pkA, pkR\n",
    "\n",
    "\n",
    "\n",
    "    def assemble_placekey(pkA, pkR):\n",
    "        placekey_df = pd.concat([pkA, pkR], ignore_index=True)\n",
    "        placekey_df.shape, placekey_df.pkwhat.count() # ((85915, 45), 85915)\n",
    "        # IMPORTANT: SAVE PLACEKEY responses!\n",
    "        placekey_df.to_feather(f'placekeyed/{date}_{sf_code}.ftr')\n",
    "        # remove unique placekey values (unique addresses)\n",
    "        placekey_df = placekey_df[placekey_df.duplicated(subset=['placekey'], keep=False)].copy()\n",
    "        placekey_df.replace(\"\", np.nan, inplace=True)\n",
    "        placekey_df.sort_values(['pkwhere', 'pkwhat'], inplace=True)\n",
    "        print(f\"End of Placekey API: reconciled {len(placekey_df)} addresses, \\\n",
    "         into {placekey_df.placekey.nunique()} different placekey groups\")\n",
    "\n",
    "        return placekey_df\n",
    "\n",
    "    def pip(attom, regrid, legal_matched, num_matched, placekey_df):\n",
    "        print(f\"Start PIP step for state {sf_code}\")\n",
    "        ## MATCH 4 - PIP: attom points in regrid polygons\n",
    "        attom4 = attom[~attom.aid.isin(\n",
    "            set(legal_matched.aid).union(set(num_matched.aid)).union(set(placekey_df.aid))\n",
    "        )]\n",
    "        regrid4 = regrid[~regrid.rid.isin(\n",
    "            set(legal_matched.rid).union(set(num_matched.rid)).union(set(placekey_df.rid))\n",
    "        )]\n",
    "        # REGRID 3.2mil down to 150k;  ATTOM 3.3mil down to 360k\n",
    "        # DC jun14: regrid 140k to 2k, attom 210k to 4k\n",
    "        regrid.shape, attom.shape, regrid4.shape, attom4.shape, \n",
    "\n",
    "        # ATTOM GEOMETRY TYPE\n",
    "        attom4_geo = gp.GeoDataFrame(\n",
    "                attom4, geometry=gp.points_from_xy(attom4.Lon, attom4.Lat)).set_crs(CRS_REGRID_PARCELS)\n",
    "        # REGRID GEOMETRY TYPE\n",
    "        list_of_regrid_feathers = glob(f\"attom/data/regrid-feather/{sf_code.lower()}_*.ftr\")#[-1:] # read regrid wkt files\n",
    "        with Pool(16) as pool:\n",
    "            regrid_wkt = pool.map(\n",
    "                lambda feather : pd.read_feather(feather, columns=['ll_uuid', 'geometry']), \n",
    "                list_of_regrid_feathers)\n",
    "            regrid_wkt = pd.concat(regrid_wkt, ignore_index=True)    \n",
    "\n",
    "        regrid_wkt.rename(columns={'geometry': 'geom', 'll_uuid': 'rid'}, inplace=True)    \n",
    "        # convert pandas df to geopandas df: 1min\n",
    "        regrid_wkt = gp.GeoDataFrame(regrid_wkt, \n",
    "                                      geometry=gp.GeoSeries.from_wkt(regrid_wkt.geom, crs = CRS_REGRID_PARCELS))\n",
    "        regrid_wkt = regrid_wkt.drop(columns = 'geom')\n",
    "        # USE regrid_wkt.merge here, in order to retain geopandas df and CRS \n",
    "        regrid4_geo = regrid_wkt.merge(regrid4, on = 'rid', how='right')\n",
    "        regrid4.shape, regrid4_geo.shape, type(regrid4_geo)\n",
    "\n",
    "        # SPATIAL JOIN\n",
    "        # `inner`: drop anything that didn't contain-within\n",
    "        # RETAIN df_left geometry (regrid parcel polygon)\n",
    "        df_pip = regrid4_geo.sjoin(attom4_geo, how=\"inner\", predicate='contains')\n",
    "        df_pip['geom'] = df_pip.geometry.to_wkt()\n",
    "        df_pip.shape\n",
    "        return attom4, regrid4, df_pip\n",
    "\n",
    "    def h3hex(attom4, regrid4, df_pip):\n",
    "        ## MATCH 5 - h3 hex @res13\n",
    "        df_last = pd.concat([attom4[~attom4.aid.isin(df_pip.aid)], \n",
    "                            regrid4[~regrid4.rid.isin(df_pip.rid)]], ignore_index= True)\n",
    "        #### hex13 col is union of rhex13 and ahex13\n",
    "        df_last['hex13'] = np.where(df_last.rhex13.notna(), df_last.rhex13, df_last.ahex13)\n",
    "\n",
    "        # THESE ARE DUPLICATES: repeated hex13\n",
    "        df_h3_dup = df_last[df_last.duplicated(subset=['hex13'], keep=False)].copy()\n",
    "        df_h3_dup = df_h3_dup.query('hex13 != \"0\"')\n",
    "\n",
    "        # df_rem also contains df_rem_nocoords (below) \n",
    "        # For now, consider them as new parcels\n",
    "        df_last = df_last[~df_last.hex13.isin(df_h3_dup.hex13)] # i.e. drop_duplicates(subset=['hex13'], keep=False)\n",
    "        return df_h3_dup, df_last\n",
    "\n",
    "    def gen_attom_landuse():\n",
    "        # ATTOM: state landuse df: maps aid to our standardized landuse\n",
    "        with Pool(16) as pool:\n",
    "            alanduse = pool.map(\n",
    "                lambda feather_file : pd.read_feather(\n",
    "                    feather_file, columns=['[ATTOM ID]', 'PropertyUseGroup',  'PropertyUseStandardized',]), \n",
    "                glob(f\"attom/data/attom-feather/{sf_code.upper()}_*.ftr\"))\n",
    "            alanduse = pd.concat(alanduse, ignore_index=True)    \n",
    "\n",
    "        alanduse.columns = ['aid', 'group', \n",
    "                            'code', # TODO LATER: use to identify MDU / subclass CAI (into edu/health/gov/commu)\n",
    "                           ]\n",
    "        # HOTFIX: group has mixed cases (e.g. both Commercial and COMMERCIAL present in ATTOM raw data)\n",
    "        alanduse['group'] = alanduse.group.str.upper()\n",
    "\n",
    "        # re-mapping aid-landuse\n",
    "        conditions = [ \n",
    "            alanduse.group.eq('AGRICULTURE / FARMING'),\n",
    "            alanduse.group.eq('RESIDENTIAL'),    \n",
    "            alanduse.group.eq('VACANT LAND'),    \n",
    "            alanduse.group.eq('PUBLIC WORKS'),    \n",
    "            alanduse.group.isin(['INDUSTRIAL', 'COMMERCIAL']), ]\n",
    "        choices = ['farm', 'resi', 'vacland', 'CAI', 'biz']\n",
    "        alanduse['alanduse'] = np.select(conditions, choices, default='rem')\n",
    "        alanduse.alanduse.value_counts()\n",
    "        return alanduse\n",
    "\n",
    "    def gen_regrid_landuse():\n",
    "        # REGRID state landuse: maps rid to our standardized landuse\n",
    "        with Pool(16) as pool:\n",
    "            rlanduse = pool.map(\n",
    "                lambda feather_file : pd.read_feather(\n",
    "                    feather_file, columns=['ll_uuid', 'lbcs_activity', 'lbcs_function', 'lbcs_structure', 'lbcs_site', ]), \n",
    "                glob(f\"attom/data/regrid-feather/{sf_code.lower()}_*.ftr\"))\n",
    "            rlanduse = pd.concat(rlanduse, ignore_index=True)    \n",
    "\n",
    "        rlanduse.columns = ['rid', 'acti', 'func', 'struc', 'site']\n",
    "        # Optimistic: assume parcels with missing lbcs_site: has building onsite i.e. = 6500\n",
    "        # WORKAROUND: .astype(float) regress and fill in NaN, NEED TO chain fillna(6500) before coerce to int\n",
    "        rlanduse['site'] = rlanduse.site.fillna(\"6500\").astype(float).fillna(6500).astype(int) // 1000\n",
    "        # NOTE: With structure iff lbcs_site = 6k\n",
    "        rlanduse[['acti', 'struc', 'func']] = rlanduse[['acti', 'struc', 'func']].fillna(\"0\").astype(float).fillna(0).astype(int) // 1000\n",
    "        rlanduse['rlanduse']= np.vectorize(regrid_landuse_classifier)(\n",
    "            rlanduse['acti'], rlanduse['func'],  rlanduse['struc'], rlanduse['site'])\n",
    "\n",
    "        rlanduse.rlanduse.value_counts()\n",
    "        rlanduse.shape \n",
    "        return rlanduse\n",
    "\n",
    "    def assemble_uniq_identifier_df(legal_matched, legal_diff, num_matched, num_diff, alanduse, rlanduse, placekey_df, df_h3_dup, df_pip, df_last):\n",
    "        # Exclude legal_diff (which later had been reviewed in Placekey step) from legal_matched\n",
    "        # similarly, exclude num_diff from num_matched\n",
    "        uniq_identifier_matched = pd.concat([\n",
    "            legal_matched[~legal_matched.index.isin(legal_diff.index)],\n",
    "            num_matched[~num_matched.index.isin(num_diff.index)],], ignore_index = True)[rad_in_cols]\n",
    "        uniq_identifier_matched = uniq_identifier_matched.merge(alanduse[['aid', 'alanduse']], on='aid', how='left')\n",
    "        uniq_identifier_matched = uniq_identifier_matched.merge(rlanduse[['rid', 'rlanduse']], on='rid', how='left')\n",
    "        if len(uniq_identifier_matched):\n",
    "            uniq_identifier_matched['landuse'] = np.vectorize(rad_landuse_classifier_single)(uniq_identifier_matched['alanduse'], uniq_identifier_matched['rlanduse'])\n",
    "        else:\n",
    "            uniq_identifier_matched['landuse'] = \"None\"\n",
    "        # NOTE: as of now, 2 businesses with different SUITES are mapped to same placekey!\n",
    "        # TODO LATER: need to consider addressSub to expand a address1 into multiple addresses, if possible! (and AddressSub to expand Address1) \n",
    "        pk_group = placekey_df.copy(deep = True)[rad_in_cols + ['placekey']]\n",
    "        # each row (keyed on hex13) comes from either REGRID(has rid), or ATTOM(has aid)\n",
    "        h3_group = df_h3_dup.copy(deep = True)[rad_in_cols + ['hex13']]\n",
    "\n",
    "        for GROUPBY, group_df in zip(['placekey', 'hex13'], [pk_group, h3_group]):\n",
    "            group_df = group_df.merge(alanduse[[\n",
    "                'aid', 'alanduse']], on='aid', how='left').merge(rlanduse[[\n",
    "                'rid', 'rlanduse']], on='rid', how='left')\n",
    "            group_df = group_df.groupby(GROUPBY).agg({col : list for col in rad_in_cols + ['alanduse', 'rlanduse']})\n",
    "            # Combine 2 list columns with + operator\n",
    "            if len(group_df):\n",
    "                group_df['landuse'] = np.vectorize(rad_landuse_classifier_many)(group_df['alanduse'] + group_df['rlanduse'])\n",
    "            else:\n",
    "                group_df['landuse'] = \"None\"            \n",
    "            # Assign back to variables\n",
    "            if GROUPBY == 'placekey':    pk_group = group_df # (142625, 17)\n",
    "            else:                        h3_group = group_df # (5002, 17)\n",
    "\n",
    "        # each row (keyed on rid) contains 1 rid and 1 aid\n",
    "        pip_group_prev = df_pip.copy(deep=True)[rad_in_cols]\n",
    "        pip_group_prev = pip_group_prev.merge(alanduse[['aid', 'alanduse']], on='aid', how='left')\n",
    "        pip_group_prev = pip_group_prev.merge(rlanduse[['rid', 'rlanduse']], on='rid', how='left')\n",
    "\n",
    "        attom_pip_agg_cols = ['aid', 'Address1', 'AddressSub', 'Owner', 'Owner2', 'Lat', 'Lon']\n",
    "        pip_group = pip_group_prev.groupby('rid').agg({ col : list for col in attom_pip_agg_cols + ['alanduse']})\n",
    "        pip_group_prev.set_index('rid', inplace=True)\n",
    "        # drop duplicate indices\n",
    "        pip_group_prev = pip_group_prev[~pip_group_prev.index.duplicated(keep='first')][\n",
    "            ['rlanduse', 'address1', 'addressSub', 'owner', 'owner2', 'lat', 'lon']] \n",
    "        # excludes rid, includes rlanduse: turn a regular column into a list column\n",
    "        for col in pip_group_prev.columns:\n",
    "            pip_group_prev[col] = pip_group_prev[col].map(lambda x: [x])  \n",
    "\n",
    "        pip_group = pip_group.join(pip_group_prev, how = 'left') \n",
    "        if len(pip_group):\n",
    "            pip_group['landuse'] = np.vectorize(rad_landuse_classifier_many)(pip_group['alanduse'] + pip_group['rlanduse'])  \n",
    "        else:\n",
    "            pip_group['landuse'] = \"None\"                    \n",
    "        ### enchance / add more fields to RAD2\n",
    "        # copy rid to rId\n",
    "        uniq_identifier_matched['rId'] = uniq_identifier_matched[['rid']]\n",
    "        # 5 new columns: pId, pAddress1, pAddressSub, pLat, pLon\n",
    "        conds = uniq_identifier_matched['address1'].isna() & uniq_identifier_matched['Address1'].notna()\n",
    "        conds_ndarr = np.tile(conds.values[:, None], 3) # repeat conds values 3 times\n",
    "        uniq_identifier_matched[['pId', 'pAddress1', 'pAddressSub', ]] = np.where(conds_ndarr, \n",
    "                                uniq_identifier_matched[['aid', 'Address1', 'AddressSub', ]],\n",
    "                                uniq_identifier_matched[['rid', 'address1', 'addressSub', ]],)   \n",
    "        uniq_identifier_matched['pLat'] = np.where(uniq_identifier_matched['lat'].isna() & uniq_identifier_matched['Lat'].notna(), \n",
    "                                         uniq_identifier_matched['Lat'], uniq_identifier_matched['lat'])\n",
    "        uniq_identifier_matched['pLon'] = np.where(uniq_identifier_matched['lon'].isna() & uniq_identifier_matched['Lon'].notna(), \n",
    "                                         uniq_identifier_matched['Lon'], uniq_identifier_matched['lon'])\n",
    "        # Hotfix bad owner data: has owner2, but not owner!\n",
    "        uniq_identifier_matched['owner'] = np.where(uniq_identifier_matched['owner'].isna(), uniq_identifier_matched['owner2'], uniq_identifier_matched['owner'])\n",
    "        uniq_identifier_matched['Owner'] = np.where(uniq_identifier_matched.Owner.isna(), uniq_identifier_matched.Owner2, uniq_identifier_matched.Owner)\n",
    "        # Select pOwner from regrid owner or attom Owner\n",
    "        uniq_identifier_matched['pOwner'] = np.where(uniq_identifier_matched['owner'].notna(), uniq_identifier_matched['owner'], uniq_identifier_matched['Owner'])\n",
    "        # ASIDE: #rows with no owners\n",
    "        # uniq_identifier_matched.query('pOwner != pOwner')[['owner', 'owner2', 'Owner', 'Owner2', 'pOwner']].shape\n",
    "\n",
    "        # pk_group ~ h3_group\n",
    "        if len(pk_group):\n",
    "            pk_group['fields'] =  np.vectorize(rad_fields_vec)(\n",
    "                pk_group['rid'],    pk_group['aid'],    pk_group['address1'],    pk_group['addressSub'],    \n",
    "                pk_group['Address1'],    pk_group['AddressSub'],    pk_group['lat'],    pk_group['lon'],    \n",
    "                pk_group['Lat'],    pk_group['Lon'],    \n",
    "                pk_group['owner'],    pk_group['owner2'],    pk_group['Owner'],    pk_group['Owner2'],)\n",
    "            # SPLIT fields into subfields\n",
    "            pk_group[rad_out_cols] = pk_group.fields.str.split(\"\\t\", expand=True).replace([\"None\", \"nan\"], np.nan)\n",
    "        else:\n",
    "            pk_group[rad_out_cols] = np.nan\n",
    "        # pk_group ~ h3_group\n",
    "        if len(h3_group):\n",
    "            h3_group['fields'] =  np.vectorize(rad_fields_vec)(\n",
    "                h3_group['rid'],    h3_group['aid'],    h3_group['address1'],    h3_group['addressSub'],    \n",
    "                h3_group['Address1'],    h3_group['AddressSub'],    h3_group['lat'],    h3_group['lon'],    \n",
    "                h3_group['Lat'],    h3_group['Lon'],    \n",
    "                h3_group['owner'],    h3_group['owner2'],    h3_group['Owner'],    h3_group['Owner2'],)\n",
    "            # SPLIT fields into subfields\n",
    "            h3_group[rad_out_cols] = h3_group.fields.str.split(\"\\t\", expand=True).replace([\"None\", \"nan\"], np.nan)\n",
    "        else:\n",
    "            h3_group[rad_out_cols] = np.nan \n",
    "\n",
    "        # pip_group is slightly different from (pk_group ~ h3_group)\n",
    "        pip_group.reset_index(inplace=True)\n",
    "        pip_group['rid'] = pip_group['rid'].map(lambda cell: [cell])\n",
    "\n",
    "        if len(pip_group):\n",
    "            pip_group['fields'] =  np.vectorize(rad_fields_vec)(\n",
    "                pip_group['rid'],    pip_group['aid'],    pip_group['address1'],    pip_group['addressSub'],    \n",
    "                pip_group['Address1'],    pip_group['AddressSub'],    pip_group['lat'],    pip_group['lon'],    \n",
    "                pip_group['Lat'],    pip_group['Lon'],    \n",
    "                pip_group['owner'],    pip_group['owner2'],    pip_group['Owner'],    pip_group['Owner2'],)\n",
    "            # SPLIT fields into subfields\n",
    "            pip_group[rad_out_cols] = pip_group.fields.str.split(\"\\t\", expand=True).replace([\"None\", \"nan\"], np.nan)\n",
    "        else:\n",
    "            pip_group[rad_out_cols] = np.nan\n",
    "\n",
    "        ### df_last (all remaining uniq demand points)\n",
    "        df_points = df_last.query('aid == aid')[\n",
    "            ['aid', 'Owner', 'Owner2', 'Address1', 'AddressSub', 'Lat', 'Lon' ,]]\n",
    "        df_points = df_points.merge(alanduse[['aid', 'alanduse']], \n",
    "                                    on='aid', how='left').rename(columns= {\n",
    "            'aid' : 'pId', 'alanduse' : 'landuse', 'Address1' : 'pAddress1',\n",
    "            'AddressSub' : 'pAddressSub', 'Lat' : 'pLat', 'Lon' : 'pLon',\n",
    "        })\n",
    "\n",
    "        df_points['pOwner'] = np.where(df_points.Owner.isna(), df_points.Owner2, df_points.Owner)\n",
    "        df_points['rId'] = np.nan\n",
    "\n",
    "\n",
    "        # regrid_geom.merge(...geometry) # if want kepler visual\n",
    "        df_polys = df_last.query('rid == rid')[\n",
    "            ['rid', 'owner', 'owner2', 'address1', 'addressSub', 'lat', 'lon']]\n",
    "        df_polys = df_polys.merge(rlanduse[['rid', 'rlanduse']], \n",
    "                                  on='rid', how='left').rename(columns= {\n",
    "            'rid' : 'pId', 'rlanduse' : 'landuse',\n",
    "            'address1' : 'pAddress1', 'addressSub' : 'pAddressSub',\n",
    "            'lat' : 'pLat', 'lon' : 'pLon',\n",
    "        })\n",
    "        df_polys['pOwner'] = np.where(df_polys.owner.isna(), df_polys.owner2, df_polys.owner)\n",
    "        df_polys['rId'] = df_polys['pId']\n",
    "\n",
    "        df_points.shape, df_polys.shape \n",
    "\n",
    "        # # ASIDE: has coords, but no address\n",
    "        # # Remained here because these points had not been mapped to any polygons in PIP step\n",
    "        # df_points.query('pAddress1 != pAddress1 & pLat == pLat').shape # (26401, 10) jun3 (26728, 10)\n",
    "\n",
    "        ### add alternative IDs\n",
    "        # add alternative ids as a list column\n",
    "        uniq_identifier_matched['altIds'] = list(zip(uniq_identifier_matched.rid, uniq_identifier_matched.aid))\n",
    "        pk_group['altIds'] = pk_group[['rid', 'aid']].apply(lambda row: [v for l in row for v in l if v is not np.nan], axis=1)\n",
    "        h3_group['altIds'] = h3_group[['rid', 'aid']].apply(lambda row: [v for l in row for v in l if v is not np.nan], axis=1)\n",
    "        pip_group['altIds'] = pip_group[['rid', 'aid']].apply(lambda row: [v for l in row for v in l if v is not np.nan], axis=1)\n",
    "        # these groups altIds is simply pId (as a list of length 1)\n",
    "        df_points['altIds'] = df_points['pId'].map(lambda cell: [cell])\n",
    "        df_polys['altIds'] = df_polys['pId'].map(lambda cell: [cell])\n",
    "\n",
    "\n",
    "        outcols = rad_out_cols + ['altIds', 'landuse']\n",
    "        rad2 = pd.concat([uniq_identifier_matched[outcols], \n",
    "                    pk_group[outcols].reset_index(),\n",
    "                    h3_group[outcols].reset_index(),\n",
    "                    pip_group[outcols], # index=rid=rId=pId\n",
    "                    df_points[outcols],\n",
    "                    df_polys[outcols],])\n",
    "        return rad2\n",
    "\n",
    "    def assemble_poi():    \n",
    "        poi = pd.read_feather(f'temp/jun10_poi_{sf_code}.ftr')#.head(300) # read from a saved feather\n",
    "        poi = poi.drop(columns=['STATENAME', 'STATE', 'INDUSTRY', 'FRANCHISE', 'PRIMARY', 'COUNTY3', 'OBID', 'GEO_MATCH_CODE_TEXT'])\n",
    "        poi = poi.query(\"BUSNAME == BUSNAME\").copy() # not nan\n",
    "        poi['NAMESTREETZIP'] = poi['BUSNAME'].str.strip() + \" @ \" + poi['STREET'].str.strip() + \" @ \" + poi['ZIP'].str.strip()\n",
    "        poi = poi.drop_duplicates([\"NAMESTREETZIP\"], keep='first').reset_index(drop=True)\n",
    "        poi.shape\n",
    "\n",
    "        cai_health, health_df = getcai(poi, \"health\")\n",
    "        cai_gov, gov_df = getcai(poi, \"government\")\n",
    "        cai_comm, comm_df = getcai(poi, \"community\")\n",
    "        cai_edu, edu_df = getcai(poi, \"education\")\n",
    "        cai_relig, relig_df = getcai(poi, \"religious\")\n",
    "        cai_outdoor, outdoor_df = getcai(poi, \"outdoor\")\n",
    "        # Dont need, @yuan what are these for?\n",
    "        del cai_health, cai_gov, cai_comm, cai_edu, cai_relig, cai_outdoor \n",
    "\n",
    "        poi[\"DPtype\"] = \"commercial\"\n",
    "        poi[\"DPtype\"] = np.where(poi[\"BUSNAME\"].isin(outdoor_df[\"BUSNAME\"]), \"outdoor\", poi[\"DPtype\"])\n",
    "        # religious POI belongs to community\n",
    "        poi[\"DPtype\"] = np.where(poi[\"BUSNAME\"].isin(relig_df[\"BUSNAME\"]), \"community\", poi[\"DPtype\"])\n",
    "        poi[\"DPtype\"] = np.where(poi[\"BUSNAME\"].isin(edu_df[\"BUSNAME\"]), \"education\", poi[\"DPtype\"])\n",
    "        poi[\"DPtype\"] = np.where(poi[\"BUSNAME\"].isin(comm_df[\"BUSNAME\"]), \"community\", poi[\"DPtype\"])\n",
    "        poi[\"DPtype\"] = np.where(poi[\"BUSNAME\"].isin(gov_df[\"BUSNAME\"]), \"government\", poi[\"DPtype\"])\n",
    "        poi[\"DPtype\"] = np.where(poi[\"BUSNAME\"].isin(health_df[\"BUSNAME\"]), \"health\", poi[\"DPtype\"])\n",
    "        poi[\"CAIsubtype\"] = np.where(poi[\"DPtype\"].isin([\"community\",\"education\",\"government\",\"health\"]), poi[\"DPtype\"], \"notCAI\")\n",
    "        poi[\"DPtype\"] = np.where(poi[\"CAIsubtype\"] == 'notCAI', poi[\"DPtype\"], \"CAI\")\n",
    "\n",
    "\n",
    "        # ASSUMPTION: no 2 biz/POIs occupies same hexagon ~ 2x3meter \n",
    "        poi = poi.rename(columns={\"LONGITUDE\" : 'lng', 'LATITUDE' : 'lat', 'DPtype': 'landuse'})\n",
    "        poi = poi.h3.geo_to_h3(POI_h3_res).reset_index()\n",
    "\n",
    "        # GENERATE ID from other values in the row\n",
    "        # concat string columns using sum(axis=1); then map to hash values, using hash function\n",
    "        poi['pId'] = poi[['NAMESTREETZIP', 'h3_14']].sum(axis=1).map(hash)\n",
    "        # confirm all unique id\n",
    "        if not (poi.shape[0] == poi.pId.count()== poi.pId.nunique()): # ((286050, 18), 286050, 286050)\n",
    "            print(\"ALERT: missing unique id\")\n",
    "\n",
    "        poi['landuse'] = poi['landuse'].replace({\"commercial\" : 'biz'})\n",
    "\n",
    "        poi_keep_cols = ['pId', 'BUSNAME', 'PHONE', \n",
    "                'STREET', 'CITY', 'ZIP', \n",
    "                'lng', 'lat', 'SIC', 'CATEGORY', \n",
    "                'landuse', 'CAIsubtype', 'h3_14',]\n",
    "        poi = poi[poi_keep_cols].rename(columns={'STREET': 'pAddress1', 'CITY': 'pCity', 'ZIP': 'pZip', \n",
    "                              'BUSNAME': 'pOwner', 'CATEGORY' : 'pIndustry', 'SIC': 'pSIC'})\n",
    "        poi['source'] = 'POI'\n",
    "        return poi\n",
    "\n",
    "    def assemble_rad3(rad2, poi):    \n",
    "        rad2_with_dup = pd.DataFrame(rad2).rename(columns={'pLat' : 'lat', 'pLon' : 'lng'})\n",
    "        rad2_with_dup[['lat', 'lng']] = rad2_with_dup[['lat', 'lng']].astype(float)\n",
    "        rad2_with_dup = rad2_with_dup.h3.geo_to_h3(POI_h3_res).reset_index()\n",
    "        rad2_with_dup['CAIsubtype'] = np.where(rad2_with_dup['landuse'] == 'CAI', 'parcelCAI', 'notCAI')\n",
    "        rad3_with_dup = pd.concat([poi, rad2_with_dup], ignore_index=True)\n",
    "        uniq_poi = rad3_with_dup.drop_duplicates(subset=[f'h3_{POI_h3_res}'], keep=False, ignore_index=True)\n",
    "        uniq_poi = uniq_poi.query('source == \"POI\"')\n",
    "        rad3 = pd.concat([uniq_poi, rad2_with_dup], ignore_index=True)\n",
    "        rad2.shape, poi.shape, rad3.shape, # ((139663, 13), (48758, 14), (142533, 20))\n",
    "        # 3_320_097 + 286_050 + h3 deduped = 3_389_890\n",
    "\n",
    "        ### JOINING RAD3 with TIGER2019/CB\n",
    "        tiger = pd.read_feather(f'tigerCB2019/{sf_code}.ftr')\n",
    "        tiger.columns = ['GEOID', 'geom']\n",
    "        # convert pandas df to geopandas df\n",
    "        tiger = gp.GeoDataFrame(tiger, \n",
    "                  geometry = gp.GeoSeries.from_wkt(tiger.geom, crs = CRS_REGRID_PARCELS))\n",
    "\n",
    "        # convert pandas df to geopandas df\n",
    "        rad3CB = gp.GeoDataFrame(rad3, \n",
    "                   geometry = gp.points_from_xy(rad3.lng, rad3.lat, crs = CRS_REGRID_PARCELS))\n",
    "\n",
    "        rad3CB = gp.sjoin(rad3CB, tiger, how=\"left\", \n",
    "            predicate='within').rename(columns={'geom': 'wktCB'}).drop(\n",
    "            columns=['index_right', 'hex13', 'geometry']).replace('nan', np.nan)\n",
    "\n",
    "        print(f\"NOTE: number of demand points fallen outside of CB boundaries = \\\n",
    "         {rad3CB.query('GEOID != GEOID').shape[0]}\")\n",
    "\n",
    "\n",
    "        ### ADD SPEED RANK\n",
    "        date_download_ES = \"jun14\"\n",
    "        ESdf = pd.read_feather(f'ESrank/{date_download_ES}_{sf_code}.ftr').rename(columns={\n",
    "                                    'speedRankReadyRaw': 'rank'})\n",
    "        # dont need demand points fallen outside state censusblock boundaries\n",
    "        rad3CB_rank = rad3CB.dropna(subset='GEOID').merge(ESdf[['GEOID', 'rank']], on='GEOID', how='left')\n",
    "\n",
    "        rad3CB_rank.shape # (3386316, 22)\n",
    "\n",
    "        ## Complete! Now SAVE TO MBTILES\n",
    "        rad3CB_rank['pId'] = rad3CB_rank['pId'].astype(str)\n",
    "        # save to ftr\n",
    "        rad3CB_rank.to_feather(f'rad3_complete/{date}_{sf_code}.ftr')\n",
    "        # save to csv\n",
    "        rad3CB_rank.drop(columns=['h3_14', 'wktCB', 'placekey']).rename(\n",
    "            columns={'rank': 'speedRankReadyRaw'}).to_csv(\n",
    "            f'rad3_complete/{date}_{sf_code}.csv', index=False)\n",
    "\n",
    "        with open('tippecanoe.cmds', 'a') as wf:\n",
    "            wf.write(f\"TIPPECANOE COMMAND for {sf_code}::: \\n\")\n",
    "            cmd = f\"/usr/local/bin/tippecanoe -zg -Z6 --extend-zooms-if-still-dropping --drop-densest-as-needed --force -o \\\n",
    "        /home/nhat/update-regrid/data/rad3_{sf_code.lower()}.mbtiles -l rad3_{sf_code.lower()} \\\n",
    "        /home/nhat/demand-points/rad3_complete/{date}_{sf_code}.csv \\n\"\n",
    "            wf.write(cmd)\n",
    "\n",
    "        return rad3CB_rank\n",
    "\n",
    "\n",
    "    # GLOBAL SCOPE    \n",
    "    POI_h3_res = 14  \n",
    "    rad_in_cols = ['aid', 'rid', \n",
    "                  'address1', 'addressSub', 'Address1', 'AddressSub',\n",
    "                  'owner', 'owner2', 'Owner', 'Owner2', \n",
    "                  'lat', 'lon', 'Lat', 'Lon', \n",
    "                   # 'landuse',  \n",
    "                  ]\n",
    "    rad_out_cols = ['pId', 'rId', 'pAddress1', 'pAddressSub', 'pOwner', 'pLat', 'pLon']\n",
    "\n",
    "    def main(sf_code):\n",
    "        regrid, attom = init()\n",
    "        legal_matched, legal_diff = legal(attom, regrid)\n",
    "        num_matched, num_diff = numb(attom, regrid, legal_matched, legal_diff)\n",
    "        pkA, pkR = placekey_query(attom, regrid, legal_matched, legal_diff, num_matched, num_diff)\n",
    "        print(\"COMPLETED querying Placekey API\")\n",
    "        placekey_df = assemble_placekey(pkA, pkR)\n",
    "        attom4, regrid4, df_pip = pip(attom, regrid, legal_matched, num_matched, placekey_df)\n",
    "        df_h3_dup, df_last = h3hex(attom4, regrid4, df_pip)\n",
    "\n",
    "        # PART II: combine, reconcile, enhance RAD, and add ATTOM POI = RAD3\n",
    "        alanduse = gen_attom_landuse()\n",
    "        rlanduse = gen_regrid_landuse()\n",
    "        #### add landuse to different groups\n",
    "        rad2 = assemble_uniq_identifier_df(legal_matched, legal_diff, num_matched, num_diff, alanduse, rlanduse, placekey_df, df_h3_dup, df_pip, df_last)\n",
    "        attom.shape, regrid.shape, rad2.shape # (3316869, 10); jun3 (3320097, 11)\n",
    "\n",
    "        # ADD SOURCE COLUMN\n",
    "        # len of pId==36 = from Regrid, under10 = from Attom\n",
    "        print(f\"diff lengths of pIds (REGRID vs ATTOM source): {set(rad2.pId.str.len())}\") # {6, 7, 8, 9, 36}\n",
    "        # REGRID = primary info from REGRID\n",
    "        # ATTOM>REGRID = both REGRID and ATTOM cover, but ATTOM has more info\n",
    "        # ATTOM = new points from ATTOM (REGRID miss-coverage)\n",
    "        rad2['source'] = np.where(rad2.pId.str.len() == 36, \n",
    "                                  'REGRID', \n",
    "                                  np.where(rad2.rId.notna(), 'ATTOM>REGRID', 'ATTOM'))\n",
    "        #### SAVING a big checkpoint: RAD2\n",
    "        rad2.reset_index(drop=True).astype(str).to_feather(f'temp/{date}_{sf_code}_rad2.ftr')\n",
    "\n",
    "        ## ADD ATTOM POI\n",
    "        poi = assemble_poi()\n",
    "        # add h3_14 column\n",
    "        rad3CB_rank = assemble_rad3(rad2, poi)\n",
    "        print(f\"COMPLETE {sf_code}, {rad3CB_rank.shape}\")\n",
    "\n",
    "\n",
    "    # call the function    \n",
    "    main(sf_code)\n",
    "\n",
    "\n",
    "\n",
    "    # STARTING QUERYING PLACEKEY API: DC, attom3 82015, and regrid3 5487 =>>  SHOULD TAKE 9 min!\n",
    "    # STARTING QUERYING PLACEKEY API: CA, attom3 2188058, and regrid3 1841795 =>> took 13min to reach Placekey step: Pk API should take 220min i.e. 3hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f53976-5b6a-48f8-8203-df8d076e43e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls -lh temp/{date}_{sf_code}_rad2.ftr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74452969-3519-40a0-a87c-f12ed37a9e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls -lh temp/*_{sf_code}_rad2.ftr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f6e6b-01c0-4136-b93d-1fbb314682fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb8354f-1d8f-4df1-bf82-5effa9cbcbc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for feather_file in glob(f\"attom/data/regrid-feather/{sf_code.lower()}_*.ftr\"):\n",
    "#     print(feather_file)\n",
    "#     df = pd.read_feather(feather_file, columns=regrid_sel_cols)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1ba65f-30de-419b-85e3-3861f90fe20f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
